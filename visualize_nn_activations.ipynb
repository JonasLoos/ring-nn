{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# ring nns with single layer (1,), (2,), (5,)\n",
    "####################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from nn import Input\n",
    "from tensor import RingTensor\n",
    "from optimizer import SGD\n",
    "from typing import cast, Callable\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "x_ = RingTensor(x[:,None])\n",
    "y = np.cos(x*pi)\n",
    "# y = x * 0\n",
    "y = np.sin(6 * x) * np.cos(3 * pi * x) + 0.2 * x**3 - 0.1 * np.sign(x)  # interesting nontrivial function\n",
    "\n",
    "\n",
    "n_neurons_list = [1, 2, 5]\n",
    "fig, axs = plt.subplots(3, len(n_neurons_list), figsize=(10, 8), squeeze=False)\n",
    "\n",
    "for run, n_neurons in enumerate(n_neurons_list):\n",
    "    # initialize model and optimizer\n",
    "    nn = Input((1,1)).ff(n_neurons).mean(axis=1, keepdims=True)\n",
    "    opt = SGD(nn, lr=0.1, lr_decay=1.)\n",
    "    nn = cast(Callable[[RingTensor], RingTensor], nn)\n",
    "\n",
    "    # initialize history tracking variables\n",
    "    ys = [np.zeros_like(y)]\n",
    "    ys += [nn(x_).real().data]\n",
    "    losses_history = []\n",
    "    grad_history = []\n",
    "    differences = []\n",
    "\n",
    "    for i in range(50):\n",
    "        # check if we're done\n",
    "        # if (ys[-1] - ys[-2]).abs().sum() < 1e-2: break\n",
    "\n",
    "        # extract individual gradients\n",
    "        tmp = []\n",
    "        for i in range(len(x)):\n",
    "            out = nn(x_)\n",
    "            assert isinstance(out, RingTensor)\n",
    "            pred = out.real().squeeze(1)\n",
    "            losses = ((pred - y)**2)\n",
    "            losses[i].backward()\n",
    "            tmp.append(nn.weights[0]._grad.tolist()[0][0])\n",
    "            nn.weights[0].reset_grad()\n",
    "        grad_history.append(tmp)\n",
    "\n",
    "        # normal optimization step\n",
    "        out = nn(x_)\n",
    "        assert isinstance(out, RingTensor)\n",
    "        pred = out.real().squeeze(1)\n",
    "        losses = ((pred - y)**2)\n",
    "        loss = losses.mean()\n",
    "        losses_history.append(losses.data)\n",
    "        loss.backward()\n",
    "        opt()\n",
    "\n",
    "        # update predictions\n",
    "        ys += nn(x_).real().data,\n",
    "        differences.append((ys[-1] - ys[-2]).abs().sum())\n",
    "\n",
    "\n",
    "\n",
    "    # plot results\n",
    "    axs[0, run].set_title(f\"#neurons: {n_neurons}\")\n",
    "    ys = ys[1:]\n",
    "    axs[0, run].plot(x, y, color='black')\n",
    "    for i, y2 in enumerate(ys):\n",
    "        color = plt.cm.autumn_r(i/len(ys))\n",
    "        axs[0, run].plot(x, y2, color=color)\n",
    "    if run == 0:\n",
    "        axs[0, run].set_ylabel(\"y\")\n",
    "    else:\n",
    "        axs[0, run].set_yticks([])\n",
    "    axs[0, run].set_xticks([])\n",
    "    axs[0, run].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    for i, losses in enumerate(losses_history):\n",
    "        axs[1, run].plot(x, losses, color=plt.cm.autumn_r(i/len(losses_history)))\n",
    "    if run == 0:\n",
    "        axs[1, run].set_ylabel(\"loss\")\n",
    "    else:\n",
    "        axs[1, run].set_yticks([])\n",
    "    axs[1, run].set_xticks([])\n",
    "    axs[1, run].set_ylim(0, 5)\n",
    "\n",
    "    axs[2, run].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "    for i, grad in enumerate(grad_history):\n",
    "        axs[2, run].plot(x, grad, color=plt.cm.autumn_r(i/len(grad_history)))\n",
    "    if run == 0:\n",
    "        axs[2, run].set_ylabel(\"grad (weight 0)\")\n",
    "    else:\n",
    "        axs[2, run].set_yticks([])\n",
    "    axs[2, run].set_xlabel(\"x\")\n",
    "    axs[2, run].set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(np.array(grad_history).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# just plotting code\n",
    "####################################\n",
    "\n",
    "\n",
    "# plot results\n",
    "ys = ys[1:]\n",
    "print(len(ys))\n",
    "plt.plot(x, y)\n",
    "for i, y2 in enumerate(ys):\n",
    "    color = plt.cm.YlOrRd(i/len(ys))\n",
    "    plt.plot(x, y2, color=color)\n",
    "plt.title(\"predictions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "for i, losses in enumerate(losses_history):\n",
    "    plt.plot(x, losses, color=plt.cm.YlOrRd(i/len(losses_history)))\n",
    "plt.title(\"losses\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(differences, c='black')\n",
    "plt.scatter(range(len(differences)), differences, c=[plt.cm.YlOrRd(i/len(differences)) for i in range(len(differences))])\n",
    "plt.title(\"differences\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"difference\")\n",
    "plt.show()\n",
    "\n",
    "for i, grad in enumerate(grad_history):\n",
    "    plt.plot(x, grad, color=plt.cm.YlOrRd(i/len(grad_history)))\n",
    "plt.title(\"grads\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"grad\")\n",
    "plt.show()\n",
    "\n",
    "print(np.array(grad_history).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# ring nn with single layer (5,)\n",
    "####################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from nn import Input\n",
    "from tensor import RingTensor\n",
    "from optimizer import SGD\n",
    "from typing import cast, Callable\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "x_ = RingTensor(x[:,None])\n",
    "y = np.cos(x*pi)\n",
    "# y = x * 0\n",
    "y = np.sin(6 * x) * np.cos(3 * pi * x) + 0.2 * x**3 - 0.1 * np.sign(x)  # interesting nontrivial function\n",
    "\n",
    "\n",
    "n_neurons = 5\n",
    "fig, axs = plt.subplots(3, n_neurons+1, figsize=(3*(n_neurons+1), 8), squeeze=False)\n",
    "\n",
    "# initialize model and optimizer\n",
    "nn_separate = Input((1,1)).ff(n_neurons)\n",
    "nn = lambda x: nn_separate(x).mean(axis=1, keepdims=True)\n",
    "opt = SGD(nn_separate, lr=0.1, lr_decay=1.)\n",
    "nn = cast(Callable[[RingTensor], RingTensor], nn)\n",
    "\n",
    "# initialize history tracking variables\n",
    "ys = [np.zeros_like(y)]\n",
    "ys += [nn(x_).real().data]\n",
    "ys_separate = []\n",
    "losses_history = []\n",
    "losses_history_separate = []\n",
    "grad_history = []\n",
    "differences = []\n",
    "\n",
    "for i in range(50):\n",
    "    # check if we're done\n",
    "    # if (ys[-1] - ys[-2]).abs().sum() < 1e-2: break\n",
    "\n",
    "    # extract individual gradients\n",
    "    grad_tmp = []\n",
    "    losses_tmp = []\n",
    "    for i in range(len(x)):\n",
    "        out = nn(x_)\n",
    "        assert isinstance(out, RingTensor)\n",
    "        pred = out.real().squeeze(1)\n",
    "        losses = ((pred - y)**2)\n",
    "        losses[i].backward()\n",
    "        losses_tmp.append(losses[i].data.item())\n",
    "        grad_tmp.append(nn_separate.weights[0]._grad.tolist()[0])\n",
    "        nn_separate.weights[0].reset_grad()\n",
    "    grad_history.append(grad_tmp)\n",
    "    losses_history_separate.append(losses_tmp)\n",
    "\n",
    "    # normal optimization step\n",
    "    out = nn(x_)\n",
    "    assert isinstance(out, RingTensor)\n",
    "    pred = out.real().squeeze(1)\n",
    "    losses = ((pred - y)**2)\n",
    "    loss = losses.mean()\n",
    "    losses_history.append(losses.data)\n",
    "    loss.backward()\n",
    "    opt()\n",
    "\n",
    "    # update predictions\n",
    "    ys += nn(x_).real().data,\n",
    "    ys_separate += nn_separate(x_).real().data,\n",
    "    differences.append((ys[-1] - ys[-2]).abs().sum())\n",
    "\n",
    "\n",
    "\n",
    "# plot results\n",
    "axs[0, 0].set_title(f\"All neurons\")\n",
    "ys = ys[1:]\n",
    "axs[0, 0].plot(x, y, color='black')\n",
    "for i, y2 in enumerate(ys):\n",
    "    color = plt.cm.autumn_r(i/len(ys))\n",
    "    axs[0, 0].plot(x, y2, color=color)\n",
    "if 0 == 0:\n",
    "    axs[0, 0].set_ylabel(\"y\")\n",
    "else:\n",
    "    axs[0, 0].set_yticks([])\n",
    "axs[0, 0].set_xticks([])\n",
    "axs[0, 0].set_ylim(-1.1, 1.1)\n",
    "\n",
    "for i, losses in enumerate(losses_history):\n",
    "    axs[1, 0].plot(x, losses, color=plt.cm.autumn_r(i/len(losses_history)))\n",
    "if 0 == 0:\n",
    "    axs[1, 0].set_ylabel(\"loss\")\n",
    "else:\n",
    "    axs[1, 0].set_yticks([])\n",
    "axs[1, 0].set_xticks([])\n",
    "axs[1, 0].set_ylim(0, 3)\n",
    "\n",
    "axs[2, 0].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "for i, grad in enumerate(grad_history):\n",
    "    axs[2, 0].plot(x, np.mean(grad, axis=1), color=plt.cm.autumn_r(i/len(grad_history)))\n",
    "if 0 == 0:\n",
    "    axs[2, 0].set_ylabel(\"grad (weight 0)\")\n",
    "else:\n",
    "    axs[2, 0].set_yticks([])\n",
    "axs[2, 0].set_xlabel(\"x\")\n",
    "axs[2, 0].set_ylim(-2.1, 2.1)\n",
    "\n",
    "\n",
    "# plot results for each neuron\n",
    "for neuron in range(1, n_neurons+1):\n",
    "    axs[0, neuron].set_title(f\"#neuron: {neuron}\")\n",
    "    axs[0, neuron].plot(x, y, color='black')\n",
    "    for i, y2 in enumerate(ys_separate):\n",
    "        color = plt.cm.autumn_r(i/len(ys_separate))\n",
    "        axs[0, neuron].plot(x, y2[:,neuron-1], color=color)\n",
    "    if neuron == 0:\n",
    "        axs[0, neuron].set_ylabel(\"y\")\n",
    "    else:\n",
    "        axs[0, neuron].set_yticks([])\n",
    "    axs[0, neuron].set_xticks([])\n",
    "    axs[0, neuron].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    # for i, losses in enumerate(losses_history_separate):\n",
    "    #     axs[1, neuron].plot(x, losses[neuron-1], color=plt.cm.autumn_r(i/len(losses_history_separate)))\n",
    "    if neuron == 0:\n",
    "        axs[1, neuron].set_ylabel(\"loss\")\n",
    "    else:\n",
    "        axs[1, neuron].set_yticks([])\n",
    "    axs[1, neuron].set_xticks([])\n",
    "    axs[1, neuron].set_ylim(0, 3)\n",
    "\n",
    "    axs[2, neuron].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "    for i, grad in enumerate(grad_history):\n",
    "        axs[2, neuron].plot(x, [g[neuron-1] for g in grad], color=plt.cm.autumn_r(i/len(grad_history)))\n",
    "    if neuron == 0:\n",
    "        axs[2, neuron].set_ylabel(\"grad (weight 0)\")\n",
    "    else:\n",
    "        axs[2, neuron].set_yticks([])\n",
    "    axs[2, neuron].set_xlabel(\"x\")\n",
    "    axs[2, neuron].set_ylim(-2.1, 2.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(np.array(grad_history).mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03822142",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# ring nn with two layers (5, 1)\n",
    "####################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "from nn import Input, RingFF, Sequential\n",
    "from tensor import RingTensor\n",
    "from optimizer import SGD\n",
    "from typing import cast, Callable\n",
    "\n",
    "n = 100\n",
    "x = np.linspace(-1, 1, n)\n",
    "x_ = RingTensor(x[:,None])\n",
    "y = np.cos(x*pi)\n",
    "# y = x * 0\n",
    "y = np.sin(6 * x) * np.cos(3 * pi * x) + 0.2 * x**3 - 0.1 * np.sign(x)  # interesting nontrivial function\n",
    "\n",
    "\n",
    "n_neurons = 5\n",
    "fig, axs = plt.subplots(3, n_neurons+2, figsize=(3*(n_neurons+2), 8), squeeze=False)\n",
    "\n",
    "# initialize model and optimizer\n",
    "layer1 = RingFF(1, n_neurons)\n",
    "layer2 = RingFF(n_neurons, 1)\n",
    "nn = Sequential([layer1, layer2])\n",
    "opt = SGD(nn, lr=0.01, lr_decay=1.)\n",
    "nn = cast(Callable[[RingTensor], RingTensor], nn)\n",
    "\n",
    "# initialize history tracking variables\n",
    "ys = [np.zeros_like(y)]\n",
    "ys += [nn(x_).real().data]\n",
    "ys_separate = []\n",
    "losses_history = []\n",
    "losses_history_separate = []\n",
    "grad_history = []\n",
    "differences = []\n",
    "\n",
    "for i in range(200):\n",
    "    # check if we're done\n",
    "    # if (ys[-1] - ys[-2]).abs().sum() < 1e-2: break\n",
    "\n",
    "    # extract individual gradients\n",
    "    grad_tmp = []\n",
    "    losses_tmp = []\n",
    "    for i in range(n):\n",
    "        out = nn(x_)\n",
    "        assert isinstance(out, RingTensor)\n",
    "        pred = out.real().squeeze(1)\n",
    "        losses = ((pred - y)**2)\n",
    "        losses[i].backward()\n",
    "        losses_tmp.append(losses[i].data.item())\n",
    "        grad_tmp.append(layer1.weights[0]._grad.tolist()[0])\n",
    "        layer1.weights[0].reset_grad()\n",
    "    grad_history.append(grad_tmp)\n",
    "    losses_history_separate.append(losses_tmp)\n",
    "\n",
    "    # normal optimization step\n",
    "    out = nn(x_)\n",
    "    assert isinstance(out, RingTensor)\n",
    "    pred = out.real().squeeze(1)\n",
    "    losses = ((pred - y)**2)\n",
    "    loss = losses.mean()\n",
    "    losses_history.append(losses.data)\n",
    "    loss.backward()\n",
    "    opt()\n",
    "\n",
    "    # update predictions\n",
    "    ys += nn(x_).real().data,\n",
    "    ys_separate += layer1(x_).real().data,\n",
    "    differences.append((ys[-1] - ys[-2]).abs().sum())\n",
    "\n",
    "\n",
    "\n",
    "# plot results\n",
    "axs[0, 0].set_title(f\"All neurons\")\n",
    "ys = ys[1:]\n",
    "axs[0, 0].plot(x, y, color='black')\n",
    "for i, y2 in enumerate(ys):\n",
    "    color = plt.cm.autumn_r(i/n)\n",
    "    axs[0, 0].plot(x, y2, color=color)\n",
    "if 0 == 0:\n",
    "    axs[0, 0].set_ylabel(\"y\")\n",
    "else:\n",
    "    axs[0, 0].set_yticks([])\n",
    "axs[0, 0].set_xticks([])\n",
    "axs[0, 0].set_ylim(-1.1, 1.1)\n",
    "\n",
    "for i, losses in enumerate(losses_history):\n",
    "    axs[1, 0].plot(x, losses, color=plt.cm.autumn_r(i/n))\n",
    "if 0 == 0:\n",
    "    axs[1, 0].set_ylabel(\"loss\")\n",
    "else:\n",
    "    axs[1, 0].set_yticks([])\n",
    "axs[1, 0].set_xticks([])\n",
    "axs[1, 0].set_ylim(0, 3)\n",
    "\n",
    "axs[2, 0].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "# for i, grad in enumerate(grad_history):\n",
    "#     axs[2, 0].plot(x, np.mean(grad, axis=1), color=plt.cm.autumn_r(i/n))\n",
    "if 0 == 0:\n",
    "    axs[2, 0].set_ylabel(\"grad\")\n",
    "else:\n",
    "    axs[2, 0].set_yticks([])\n",
    "axs[2, 0].set_xlabel(\"x\")\n",
    "axs[2, 0].set_ylim(-2.1, 2.1)\n",
    "\n",
    "\n",
    "# plot final neuron\n",
    "for neuron in range(1,2):\n",
    "    axs[0, neuron].set_title(f\"#neuron: 1.{neuron}\")\n",
    "    axs[0, neuron].plot(x, y, color='black')\n",
    "    for i, y2 in enumerate(ys):\n",
    "        color = plt.cm.autumn_r(i/n)\n",
    "        axs[0, neuron].plot(x, y2[:,neuron-1], color=color)\n",
    "    if neuron == 0:\n",
    "        axs[0, neuron].set_ylabel(\"y\")\n",
    "    else:\n",
    "        axs[0, neuron].set_yticks([])\n",
    "    axs[0, neuron].set_xticks([])\n",
    "    axs[0, neuron].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    for i, losses in enumerate(losses_history):\n",
    "        axs[1, neuron].plot(x, losses, color=plt.cm.autumn_r(i/n))\n",
    "    if neuron == 0:\n",
    "        axs[1, neuron].set_ylabel(\"loss\")\n",
    "    else:\n",
    "        axs[1, neuron].set_yticks([])\n",
    "    axs[1, neuron].set_xticks([])\n",
    "    axs[1, neuron].set_ylim(0, 3)\n",
    "\n",
    "    axs[2, neuron].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "    for i, grad in enumerate(grad_history):\n",
    "        axs[2, neuron].plot(x, np.mean(grad, axis=1), color=plt.cm.autumn_r(i/n))\n",
    "    if neuron == 0:\n",
    "        axs[2, neuron].set_ylabel(\"grad\")\n",
    "    else:\n",
    "        axs[2, neuron].set_yticks([])\n",
    "    axs[2, neuron].set_xlabel(\"x\")\n",
    "    axs[2, neuron].set_ylim(-2.1, 2.1)\n",
    "\n",
    "\n",
    "\n",
    "# plot results for each neuron\n",
    "for neuron in range(1, n_neurons+1):\n",
    "    axs[0, neuron+1].set_title(f\"#neuron: 0.{neuron}\")\n",
    "    axs[0, neuron+1].plot(x, y, color='black')\n",
    "    for i, y2 in enumerate(ys_separate):\n",
    "        color = plt.cm.autumn_r(i/n)\n",
    "        axs[0, neuron+1].plot(x, y2[:,neuron-1], color=color)\n",
    "    if neuron == 0:\n",
    "        axs[0, neuron+1].set_ylabel(\"y\")\n",
    "    else:\n",
    "        axs[0, neuron+1].set_yticks([])\n",
    "    axs[0, neuron+1].set_xticks([])\n",
    "    axs[0, neuron+1].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    # for i, losses in enumerate(losses_history_separate):\n",
    "    #     axs[1, neuron+1].plot(x, losses[neuron-1], color=plt.cm.autumn_r(i/n))\n",
    "    if neuron == 0:\n",
    "        axs[1, neuron+1].set_ylabel(\"loss\")\n",
    "    else:\n",
    "        axs[1, neuron+1].set_yticks([])\n",
    "    axs[1, neuron+1].set_xticks([])\n",
    "    axs[1, neuron+1].set_ylim(0, 3)\n",
    "\n",
    "    axs[2, neuron+1].plot(x, np.zeros_like(x), color='gray', linestyle='--')\n",
    "    for i, grad in enumerate(grad_history):\n",
    "        axs[2, neuron+1].plot(x, [g[neuron-1] for g in grad], color=plt.cm.autumn_r(i/n))\n",
    "    if neuron == 0:\n",
    "        axs[2, neuron+1].set_ylabel(\"grad\")\n",
    "    else:\n",
    "        axs[2, neuron+1].set_yticks([])\n",
    "    axs[2, neuron+1].set_xlabel(\"x\")\n",
    "    axs[2, neuron+1].set_ylim(-2.1, 2.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print(np.array(grad_history).mean(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
